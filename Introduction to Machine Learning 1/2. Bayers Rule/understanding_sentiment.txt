Last week, Turing questioned whether behavioral success is enough for thinking. Searle¡¯s Chinese Room proposes the opposite argument: syntax is not semantics. Implementation of a program on symbols alone does not bring in meaning or intentionality. I am largely convinced by the assertion of Searle. Even though it is true that programs can faithfully replicate input-output patterns, there is nothing in the coded procedures that give the manipulated symbols semantic reference. However, in machine learning there can be a number of levels between mechanical rule-following and true understanding, particularly when human interpretation influences the way model outputs are exploited. The four cases in question 2-5 relates to those levels. In this perspective, I now evaluate the four cases.
In this week¡¯s assignment, the Naive Bayes classifier tokenizes tweets, counts and proceeds by applying the Bayes¡¯ rule to choose a label. It does not have access to the hidden or cultural meanings of words, nor can it analyze irony, changing context, or pragmatics, as those are associated with interconnection of tokens in the training material. Its success is only through statistical correlations in symbols. This is syntactic processing of a paradigmatic nature without semantics according to Searle. And in my view, it also lacks sentiment.
Returning to us human reading and classifying tweets. Our judgments have intentional states aimed directly at meanings: we relate expressions to background knowledge, social norms, and expectations about speakers; we can also justify our ratings and reevaluate them when being criticized. These are not merely pattern matching. They are good example of the kind of semantic understanding which Searle believes is lacking in programmed systems. So I believe a human reader does understand tweet¡¯s sentiment.
When I set my personal conception of sentiment aside, and simply seek to reflect the labeling conventions of the 1.6 million tweets dataset, my motives become pragmatic: I attempt to deduce the conventions of the annotators. For example, acknowledging that the statement ¡®not bad¡¯ is commonly noted as positive, and then I strive to maximize agreement with annotators¡®s judgments. By so doing, the task is transformed into an operational one, one fully characterized by the dataset. This is not to say that I really understand the sentiment like the common understanding, but just that I am able to describe, implement and justify the identified construct of sentiment that exists within the dataset. I will also be able to account for edge cases and resolve the disagreements which occur. As a result, what I achieve is the version of sentiment in the dataset, and not a complete understanding of sentiment itself.
In the case where a sentiment analysis model is combined with a programmer who has full access to its internal processes, it differs from the isolated Chinese Room case presented by Searle. The human, already endowed with understanding, is able to check probabilities, feature weights and errors, relate his results to linguistic evidence, and to change interpretations when discrepancies occur. Structured computation is provided by the model and semantic grounding, methodological supervision and ability to justify or rethink is provided by human. The system is thus shown to be aware of labeled sentiment, not in the sense that the program learns semantics itself, but that through the human interpretation, its process of reasoning has become consistent and justifiable.
Overall, I believe Searle is right that an isolated program does not understand. However in machine learning practice, interpretations tend to be attached to operationalized goals and at human-model processes that make statistical outputs meaningful with a social, revisible explanation. Humans know, humans who are label imitators know a dataset construct, and human plus programmed model know can show understanding by adding formal computations to reason-giving inquiry.
