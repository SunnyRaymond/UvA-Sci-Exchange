Algorithmic Bias in Hiring: Case Study on Amazon¡¯s Recruitment Tool
Artificial intelligence has been applied in recent years to aid human decision-making in various areas, ranging from finance to healthcare. Recruitment has been one such prominent field to apply machine learning algorithms, as companies used algorithms to filter resumes and find favorable candidates. Although this can be seen as an assurance of efficiency and objectivity, the case of an algorithmic hiring system that Amazon used demonstrates that such systems can replicate, and even amplify, human biases. I will review Amazon¡¯s recruitment tool, describe what kind of bias it represents, and think about how algorithmic decision making can be applied fairly.
In 2018, it was disclosed that Amazon secretly discarded an in-house AI program used to automatically evaluate job seekers. The system was trained with ten years of historical data on hiring and overwhelmingly indicated that the company in the past had hired male candidates more frequently in technical positions. Consequently, the model learned to downgrade those resumes containing indicators of being female, with key words like ¡°women¡¯s chess club¡± or the names of all women¡¯s colleges (BBC News, 2018b). Although engineers tried to modify the system, the underlying bias in the training data persisted, therefore the tool was not applicable to fair employment.
In terms of algorithmic bias classification, the case is a definite example of historic bias. Unlike in selection bias, where data are simply not representative, historic bias is due to the trends of inequality in the social and organizational history of the formation of the data. The training corpus, in the case of Amazon, was a gender imbalance of the within Amazon¡¯s own prior hiring practices. The algorithm integrated these patterns into its predictions thus strengthening rather than reducing discrimination.
The effects of the bias are significant. The deployment of the system at scale would prevent eligible female applicants from opportunities in a systematic way, which would have reinforced traditional gender gap that AI was promoted as solving. In addition to individual harm, biased recruitment algorithms may hurt the reputation of an institution and undermine the use of AI in general. This reflects concerns expressed in other algorithmic contexts, including ProPublica¡¯s analysis of COMPAS risk scores in criminal justice, where systems have been demonstrated to recreate racial inequities (Mattu, 2023). 
Nevertheless, one should learn from the failure of Amazon. First, it demonstrates that algorithmic neutrality is fairly nonsense, with data driven systems mirroring the values, histories as well as frameworks of the environments that generate their training information. Second, it shows the significance of bias auditing and monitoring prior to deployment of AI in sensitive areas. Rebalancing datasets, fairness constraints in training models, testing disparate effects in demographic categories are all techniques which can ease these issues. Third, it highlights the importance of human control. Organizations ought to employ AI as an aid and keep accountability and transparency in decisions, as opposed to fully delegating it to a black box. 
In conclusion, the recruitment algorithm by Amazon is a typical algorithmic bias of how machine learning can reinforce existing social disparities when trained with biased historical data. The case represents historical bias and highlights the dangers of using AI in situation with significant ethical consequences. To proceed, businesses should understand that fairness in AI does not come naturally but should be proactively designed, tested, and sustained. It is only at this point that algorithmic decision-making can be used to enhance equity instead of negating it.
References
BBC News. (2018b, October 10). Amazon scrapped ¡°sexist AI¡± tool. https://www.bbc.com/news/technology-45809919
Mattu, J. L. a. K. (2023, December 20). How we analyzed the COMPAS Recidivism Algorithm. ProPublica. https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm
